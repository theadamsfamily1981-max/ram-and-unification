// gpu/avatar_inference.cu
//
// Skeleton CUDA C++ for RTX 5060 Ti animation interface.
// - Uses FP16 (half) inputs/outputs.
// - Shows pinned host buffer, device buffers, and a simple kernel.
// Replace the dummy math with a real Audio2Face / Livatar-style model
// (likely via TensorRT) later.

#include <cstdio>
#include <cuda_runtime.h>
#include <cuda_fp16.h>

#define CUDA_CHECK(expr)                                                      \
    do {                                                                      \
        cudaError_t _err = (expr);                                            \
        if (_err != cudaSuccess) {                                            \
            fprintf(stderr, "CUDA error %s:%d: %s\n",                         \
                    __FILE__, __LINE__, cudaGetErrorString(_err));           \
        }                                                                     \
    } while (0)

// Simple FP16 kernel stub: phoneme_features â†’ animation parameters.
// Real implementation would apply a learned model here.
__global__
void audio2anim_kernel(const half* __restrict__ phoneme_features,
                       half* __restrict__ anim_out,
                       int n_frames,
                       int feature_dim,
                       int anim_dim) {
    int frame_idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (frame_idx >= n_frames) return;

    const half* in_row  = phoneme_features + frame_idx * feature_dim;
    half*       out_row = anim_out         + frame_idx * anim_dim;

    // Dummy transform: sum features and write to all anim dims.
    float accum = 0.0f;
    for (int i = 0; i < feature_dim; ++i) {
        accum += __half2float(in_row[i]);
    }
    half val = __float2half(accum / (float)feature_dim);

    for (int j = 0; j < anim_dim; ++j) {
        out_row[j] = val;
    }
}

// Simple host-side wrapper for one chunk
struct AvatarInferenceContext {
    // Tunable sizes matching FPGA chunk shape
    int n_frames;
    int feature_dim;
    int anim_dim;

    // Pinned host buffer for features (filled by DMA / PCIe from FPGA)
    half* h_features_pinned;

    // Device buffers
    half* d_features;
    half* d_anim;
};

bool init_avatar_inference(AvatarInferenceContext &ctx,
                           int n_frames,
                           int feature_dim,
                           int anim_dim) {
    ctx.n_frames    = n_frames;
    ctx.feature_dim = feature_dim;
    ctx.anim_dim    = anim_dim;

    size_t feat_bytes = (size_t)n_frames * feature_dim * sizeof(half);
    size_t anim_bytes = (size_t)n_frames * anim_dim    * sizeof(half);

    // Pinned host memory for DMA target
    CUDA_CHECK(cudaHostAlloc((void**)&ctx.h_features_pinned,
                             feat_bytes,
                             cudaHostAllocDefault));
    if (!ctx.h_features_pinned) return false;

    CUDA_CHECK(cudaMalloc((void**)&ctx.d_features, feat_bytes));
    CUDA_CHECK(cudaMalloc((void**)&ctx.d_anim,     anim_bytes));

    return (ctx.d_features != nullptr && ctx.d_anim != nullptr);
}

void destroy_avatar_inference(AvatarInferenceContext &ctx) {
    if (ctx.h_features_pinned) CUDA_CHECK(cudaFreeHost(ctx.h_features_pinned));
    if (ctx.d_features)        CUDA_CHECK(cudaFree(ctx.d_features));
    if (ctx.d_anim)            CUDA_CHECK(cudaFree(ctx.d_anim));
}

bool run_avatar_chunk(AvatarInferenceContext &ctx,
                      cudaStream_t stream) {
    const size_t feat_bytes = (size_t)ctx.n_frames * ctx.feature_dim * sizeof(half);
    const size_t anim_bytes = (size_t)ctx.n_frames * ctx.anim_dim    * sizeof(half);

    // At this point, ctx.h_features_pinned should already be filled by DMA
    // from the FPGA. Here we just push it to device and launch the kernel.

    CUDA_CHECK(cudaMemcpyAsync(ctx.d_features,
                               ctx.h_features_pinned,
                               feat_bytes,
                               cudaMemcpyHostToDevice,
                               stream));

    int threads = 128;
    int blocks  = (ctx.n_frames + threads - 1) / threads;

    audio2anim_kernel<<<blocks, threads, 0, stream>>>(
        ctx.d_features,
        ctx.d_anim,
        ctx.n_frames,
        ctx.feature_dim,
        ctx.anim_dim
    );

    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaStreamSynchronize(stream));

    // In a real pipeline, you'd either:
    // - keep d_anim for direct use by a renderer, or
    // - copy back to host or a graphics API buffer.
    // Here we just return true, assuming the renderer sees d_anim.
    (void)anim_bytes;
    return true;
}

// Optional minimal test entry point; safe to remove in production.
#ifdef AVATAR_INFERENCE_STANDALONE_TEST
int main() {
    AvatarInferenceContext ctx{};
    if (!init_avatar_inference(ctx, 32, 64, 50)) {
        fprintf(stderr, "Failed to init avatar inference context\n");
        return 1;
    }

    // Fake fill features
    for (int i = 0; i < 32 * 64; ++i) {
        ctx.h_features_pinned[i] = __float2half(1.0f);
    }

    cudaStream_t stream;
    CUDA_CHECK(cudaStreamCreate(&stream));

    run_avatar_chunk(ctx, stream);

    CUDA_CHECK(cudaStreamDestroy(stream));
    destroy_avatar_inference(ctx);
    return 0;
}
#endif
